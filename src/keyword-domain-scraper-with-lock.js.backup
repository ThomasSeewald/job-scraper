const puppeteer = require('puppeteer');
const { Pool } = require('pg');
const fs = require('fs');
const path = require('path');

// Database connection
const config = JSON.parse(fs.readFileSync(path.join(__dirname, '../config/database.json'), 'utf8'));
const pool = new Pool(config.production);

// German keyword mappings with English equivalents
const KEYWORD_MAPPINGS = {
    'impressum': ['impressum', 'imprint', 'legal-notice', 'legal'],
    'kontakt': ['kontakt', 'contact', 'contact-us', 'kontaktieren'],
    'karriere': ['karriere', 'career', 'careers', 'jobs', 'stellenangebote'],
    'jobs': ['jobs', 'stellenangebote', 'stellen', 'karriere', 'career', 'careers']
};

// Email regex pattern
const EMAIL_PATTERN = /[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}/g;

class KeywordDomainScraperWithLock {
    constructor(options = {}) {
        this.headless = options.headless !== false;
        this.timeout = options.timeout || 30000;
        this.maxDomains = options.maxDomains || 25;
        this.workerId = options.workerId || process.pid; // Use process ID as worker ID
        this.browser = null;
        this.page = null;
    }

    async init() {
        console.log(`[Worker ${this.workerId}] Initializing keyword domain scraper...`);
        this.browser = await puppeteer.launch({
            headless: this.headless,
            args: [
                '--no-sandbox',
                '--disable-setuid-sandbox',
                '--disable-dev-shm-usage',
                '--disable-accelerated-2d-canvas',
                '--no-first-run',
                '--no-zygote',
                '--disable-gpu'
            ]
        });
        this.page = await this.browser.newPage();
        
        await this.page.setUserAgent('Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36');
        await this.page.setDefaultTimeout(this.timeout);
        await this.page.setDefaultNavigationTimeout(this.timeout);
        
        console.log(`[Worker ${this.workerId}] Initialization complete`);
    }

    async getAndLockDomainsToScrape() {
        // Use a transaction to atomically select and lock domains
        const client = await pool.connect();
        try {
            await client.query('BEGIN');
            
            // Select domains that haven't been attempted and lock them
            const selectQuery = `
                SELECT 
                    da.id,
                    da.domain,
                    da.base_domain,
                    da.frequency
                FROM domain_analysis da
                WHERE (da.email_extraction_attempted IS NULL OR da.email_extraction_attempted = false)
                AND da.domain IS NOT NULL 
                AND da.domain <> ''
                ORDER BY da.frequency DESC, da.id
                LIMIT $1
                FOR UPDATE SKIP LOCKED
            `;
            
            const result = await client.query(selectQuery, [this.maxDomains]);
            
            if (result.rows.length > 0) {
                // Mark these domains as being processed to prevent other workers from taking them
                const domainIds = result.rows.map(row => row.id);
                const updateQuery = `
                    UPDATE domain_analysis 
                    SET notes = $1, updated_at = NOW()
                    WHERE id = ANY($2::int[])
                `;
                
                await client.query(updateQuery, [
                    `Processing by worker ${this.workerId}`,
                    domainIds
                ]);
            }
            
            await client.query('COMMIT');
            
            console.log(`[Worker ${this.workerId}] Locked ${result.rows.length} domains for processing`);
            return result.rows;
            
        } catch (error) {
            await client.query('ROLLBACK');
            console.error(`[Worker ${this.workerId}] Error locking domains:`, error);
            return [];
        } finally {
            client.release();
        }
    }

    async findKeywordLinks(domain) {
        const baseUrl = domain.startsWith('http') ? domain : `https://${domain}`;
        const keywordLinks = {};
        
        try {
            console.log(`[Worker ${this.workerId}] Visiting domain: ${baseUrl}`);
            await this.page.goto(baseUrl, { waitUntil: 'domcontentloaded' });
            await new Promise(resolve => setTimeout(resolve, 2000));
            
            const links = await this.page.evaluate(() => {
                const anchors = Array.from(document.querySelectorAll('a[href]'));
                return anchors.map(a => ({
                    href: a.href,
                    text: a.textContent.toLowerCase().trim()
                })).filter(link => link.href && link.href.startsWith('http'));
            });
            
            for (const [germanKeyword, variants] of Object.entries(KEYWORD_MAPPINGS)) {
                const matchingLinks = links.filter(link => {
                    const url = link.href.toLowerCase();
                    const text = link.text.toLowerCase();
                    
                    return variants.some(keyword => 
                        url.includes(keyword) || text.includes(keyword)
                    );
                });
                
                if (matchingLinks.length > 0) {
                    keywordLinks[germanKeyword] = matchingLinks;
                }
            }
            
            console.log(`[Worker ${this.workerId}] Found keyword links for ${domain}:`, Object.keys(keywordLinks));
            return keywordLinks;
            
        } catch (error) {
            console.log(`[Worker ${this.workerId}] Error visiting ${baseUrl}:`, error.message);
            return {};
        }
    }

    async scrapeEmailsFromKeywordPages(keywordLinks, domain) {
        const emailsByKeyword = {};
        
        for (const [keyword, links] of Object.entries(keywordLinks)) {
            const emails = new Set();
            
            for (const link of links.slice(0, 3)) {
                try {
                    console.log(`[Worker ${this.workerId}] Scraping ${keyword} page: ${link.href}`);
                    await this.page.goto(link.href, { waitUntil: 'domcontentloaded' });
                    await new Promise(resolve => setTimeout(resolve, 1000));
                    
                    const pageText = await this.page.evaluate(() => document.body.textContent || '');
                    const foundEmails = pageText.match(EMAIL_PATTERN) || [];
                    
                    foundEmails.forEach(email => {
                        if (!email.includes('example.') && 
                            !email.includes('test@') && 
                            !email.includes('noreply@') &&
                            !email.includes('no-reply@')) {
                            emails.add(email.toLowerCase());
                        }
                    });
                    
                } catch (error) {
                    console.log(`[Worker ${this.workerId}] Error scraping ${link.href}:`, error.message);
                }
            }
            
            if (emails.size > 0) {
                emailsByKeyword[keyword] = Array.from(emails).join(', ');
                console.log(`[Worker ${this.workerId}] Found ${emails.size} emails for ${keyword} on ${domain}`);
            }
        }
        
        return emailsByKeyword;
    }

    async updateDomainAnalysis(domainId, emailsByKeyword) {
        const allEmails = [];
        for (const emails of Object.values(emailsByKeyword)) {
            if (emails) {
                allEmails.push(...emails.split(', '));
            }
        }
        
        const uniqueEmails = [...new Set(allEmails)];
        const emailCount = uniqueEmails.length;
        
        const updateQuery = `
            UPDATE domain_analysis 
            SET 
                email_extraction_attempted = $1,
                emails_found = $2,
                last_extraction_date = $3,
                notes = $4,
                updated_at = $5
            WHERE id = $6
        `;
        
        let notes = '';
        if (Object.keys(emailsByKeyword).length > 0) {
            const keywordSummary = Object.keys(emailsByKeyword).join(', ');
            const emailList = uniqueEmails.slice(0, 10).join(', ');
            const truncated = uniqueEmails.length > 10 ? ` (and ${uniqueEmails.length - 10} more)` : '';
            notes = `Keyword scraping found ${emailCount} emails in: ${keywordSummary}. Emails: ${emailList}${truncated}`;
        } else {
            notes = 'Keyword scraping attempted - no emails found';
        }
        
        await pool.query(updateQuery, [
            true,
            emailCount,
            new Date(),
            notes,
            new Date(),
            domainId
        ]);
        
        console.log(`[Worker ${this.workerId}] Updated domain_analysis: ${emailCount} emails found`);
    }

    async processDomain(domainInfo) {
        const { id, domain, frequency } = domainInfo;
        
        try {
            console.log(`[Worker ${this.workerId}] Processing domain: ${domain} (frequency: ${frequency})`);
            
            const keywordLinks = await this.findKeywordLinks(domain);
            const emailsByKeyword = await this.scrapeEmailsFromKeywordPages(keywordLinks, domain);
            await this.updateDomainAnalysis(id, emailsByKeyword);
            
            const totalEmails = Object.values(emailsByKeyword).reduce((total, emails) => 
                total + (emails ? emails.split(', ').length : 0), 0);
                
            console.log(`[Worker ${this.workerId}] Completed ${domain}: ${totalEmails} emails found`);
            
            return {
                domain: domain,
                emailsFound: totalEmails,
                success: true
            };
            
        } catch (error) {
            console.error(`[Worker ${this.workerId}] Error processing ${domain}:`, error);
            
            // Mark as attempted even on error
            await this.updateDomainAnalysis(id, {});
            
            return {
                domain: domain,
                emailsFound: 0,
                success: false,
                error: error.message
            };
        }
    }

    async run() {
        try {
            console.log(`[Worker ${this.workerId}] Starting keyword domain scraping run...`);
            await this.init();
            
            const domains = await this.getAndLockDomainsToScrape();
            if (domains.length === 0) {
                console.log(`[Worker ${this.workerId}] No domains available to scrape.`);
                return;
            }
            
            const results = [];
            for (const domain of domains) {
                const result = await this.processDomain(domain);
                results.push(result);
                
                // Small delay between domains
                await new Promise(resolve => setTimeout(resolve, 1000));
            }
            
            // Summary
            const successful = results.filter(r => r.success).length;
            const totalEmails = results.reduce((sum, r) => sum + r.emailsFound, 0);
            
            console.log(`[Worker ${this.workerId}] === Keyword Scraping Summary ===`);
            console.log(`[Worker ${this.workerId}] Domains processed: ${results.length}`);
            console.log(`[Worker ${this.workerId}] Successful: ${successful}`);
            console.log(`[Worker ${this.workerId}] Total emails found: ${totalEmails}`);
            console.log(`[Worker ${this.workerId}] Average emails per domain: ${(totalEmails / results.length).toFixed(2)}`);
            
        } catch (error) {
            console.error(`[Worker ${this.workerId}] Error in keyword domain scraper:`, error);
        } finally {
            if (this.browser) {
                await this.browser.close();
            }
            await pool.end();
        }
    }
}

// CLI usage
if (require.main === module) {
    const maxDomains = process.argv[2] ? parseInt(process.argv[2]) : 25;
    const headless = process.env.HEADLESS_MODE !== 'false';
    const workerId = process.env.WORKER_ID || process.pid;
    
    const scraper = new KeywordDomainScraperWithLock({ 
        maxDomains, 
        headless,
        workerId 
    });
    
    scraper.run().catch(console.error);
}

module.exports = KeywordDomainScraperWithLock;